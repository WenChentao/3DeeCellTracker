{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7388e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CellTracker.ffn import TrainFFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d02cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 10:53:40.797548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 10:53:40.816776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 10:53:40.817882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 10:53:40.819501: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 10:53:40.821147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 10:53:40.822045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 10:53:40.822834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 10:53:41.053629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 10:53:41.054027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 10:53:41.054384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-20 10:53:41.054735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10035 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "ffn_trainer = TrainFFN(model_name=\"ffn_02\", segmentation1_path=\"./use_stardist/worm_sd02/manual_vol1/*.tif\",\n",
    "                       voxel_size=(1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b68323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|    | 0/5000 [00:00<?, ?batch/s]2023-04-20 10:53:59.813311: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Epoch 1/100: 5001batch [00:39, 126.94batch/s, Train loss=0.102]\n",
      "Epoch 2/100: 5001batch [00:39, 128.00batch/s, Train loss=0.0685]\n",
      "Epoch 3/100: 5001batch [00:39, 127.63batch/s, Train loss=0.0593]\n",
      "Epoch 4/100: 5001batch [00:39, 126.90batch/s, Train loss=0.0526]\n",
      "Epoch 5/100: 5001batch [00:39, 125.72batch/s, Train loss=0.0479]\n",
      "Epoch 6/100: 5001batch [00:40, 124.69batch/s, Train loss=0.0449]\n",
      "Epoch 7/100: 5001batch [00:38, 128.89batch/s, Train loss=0.0418]\n",
      "Epoch 8/100: 5001batch [00:40, 124.92batch/s, Train loss=0.0403]\n",
      "Epoch 9/100: 5001batch [00:39, 127.03batch/s, Train loss=0.0393]\n",
      "Epoch 10/100: 5001batch [00:39, 125.37batch/s, Train loss=0.0375]\n",
      "Epoch 11/100: 5001batch [00:41, 120.22batch/s, Train loss=0.0369]\n",
      "Epoch 12/100: 5001batch [00:41, 120.21batch/s, Train loss=0.0356]\n",
      "Epoch 13/100: 5001batch [00:41, 120.13batch/s, Train loss=0.035]\n",
      "Epoch 14/100: 5001batch [00:41, 119.64batch/s, Train loss=0.0346]\n",
      "Epoch 15/100: 5001batch [00:41, 119.95batch/s, Train loss=0.0327]\n",
      "Epoch 16/100: 5001batch [00:41, 119.91batch/s, Train loss=0.0326]\n",
      "Epoch 17/100: 5001batch [00:41, 119.67batch/s, Train loss=0.0318]\n",
      "Epoch 18/100: 5001batch [00:41, 119.91batch/s, Train loss=0.0309]\n",
      "Epoch 19/100: 5001batch [00:41, 119.72batch/s, Train loss=0.0303]\n",
      "Epoch 20/100: 5001batch [00:41, 119.82batch/s, Train loss=0.03]\n",
      "Epoch 21/100: 5001batch [00:41, 119.58batch/s, Train loss=0.0296]\n",
      "Epoch 22/100: 5001batch [00:41, 119.99batch/s, Train loss=0.029]\n",
      "Epoch 23/100: 5001batch [00:41, 119.97batch/s, Train loss=0.0284]\n",
      "Epoch 24/100: 5001batch [00:41, 119.85batch/s, Train loss=0.0279]\n",
      "Epoch 25/100: 5001batch [00:41, 119.77batch/s, Train loss=0.0269]\n",
      "Epoch 26/100: 5001batch [00:41, 121.62batch/s, Train loss=0.0266]\n",
      "Epoch 27/100: 5001batch [00:39, 126.91batch/s, Train loss=0.0264]\n",
      "Epoch 28/100: 5001batch [00:39, 126.08batch/s, Train loss=0.0258]\n",
      "Epoch 29/100: 5001batch [00:39, 126.92batch/s, Train loss=0.0257]\n",
      "Epoch 30/100: 5001batch [00:39, 126.68batch/s, Train loss=0.0246]\n",
      "Epoch 31/100: 5001batch [00:39, 126.66batch/s, Train loss=0.0242]\n",
      "Epoch 32/100: 5001batch [00:39, 126.22batch/s, Train loss=0.0242]\n",
      "Epoch 33/100: 5001batch [00:39, 126.60batch/s, Train loss=0.0237]\n",
      "Epoch 34/100: 5001batch [00:39, 127.06batch/s, Train loss=0.0239]\n",
      "Epoch 35/100: 5001batch [00:39, 126.73batch/s, Train loss=0.0238]\n",
      "Epoch 36/100: 5001batch [00:39, 126.77batch/s, Train loss=0.0227]\n",
      "Epoch 37/100: 5001batch [00:39, 127.03batch/s, Train loss=0.0236]\n",
      "Epoch 38/100: 5001batch [00:39, 126.91batch/s, Train loss=0.0225]\n",
      "Epoch 39/100: 5001batch [00:39, 126.24batch/s, Train loss=0.0228]\n",
      "Epoch 40/100: 5001batch [00:39, 126.81batch/s, Train loss=0.0219]\n",
      "Epoch 41/100: 5001batch [00:39, 126.53batch/s, Train loss=0.0221]\n",
      "Epoch 42/100: 5001batch [00:39, 126.66batch/s, Train loss=0.0224]\n",
      "Epoch 43/100: 5001batch [00:42, 117.16batch/s, Train loss=0.0222]\n",
      "Epoch 44/100: 5001batch [00:42, 116.89batch/s, Train loss=0.0218]\n",
      "Epoch 45/100: 5001batch [00:42, 116.91batch/s, Train loss=0.0217]\n",
      "Epoch 46/100: 5001batch [00:42, 116.93batch/s, Train loss=0.0209]\n",
      "Epoch 47/100: 5001batch [00:42, 117.31batch/s, Train loss=0.0209]\n",
      "Epoch 48/100: 5001batch [00:42, 117.20batch/s, Train loss=0.021]\n",
      "Epoch 49/100: 5001batch [00:42, 117.32batch/s, Train loss=0.021]\n",
      "Epoch 50/100: 5001batch [00:42, 117.07batch/s, Train loss=0.0207]\n",
      "Epoch 51/100: 5001batch [00:42, 117.25batch/s, Train loss=0.0205]\n",
      "Epoch 52/100: 5001batch [00:42, 117.22batch/s, Train loss=0.0205]\n",
      "Epoch 53/100: 5001batch [00:42, 117.04batch/s, Train loss=0.0206]\n",
      "Epoch 54/100: 5001batch [00:42, 117.12batch/s, Train loss=0.0205]\n",
      "Epoch 55/100: 5001batch [00:42, 116.66batch/s, Train loss=0.0199]\n",
      "Epoch 56/100: 5001batch [00:42, 117.14batch/s, Train loss=0.0199]\n",
      "Epoch 57/100: 5001batch [00:42, 116.71batch/s, Train loss=0.0203]\n",
      "Epoch 58/100: 5001batch [00:42, 117.04batch/s, Train loss=0.0195]\n",
      "Epoch 59/100: 5001batch [00:42, 116.44batch/s, Train loss=0.0201]\n",
      "Epoch 60/100: 5001batch [00:42, 116.79batch/s, Train loss=0.0196]\n",
      "Epoch 61/100: 5001batch [00:42, 116.80batch/s, Train loss=0.0195]\n",
      "Epoch 62/100: 5001batch [00:42, 117.36batch/s, Train loss=0.0199]\n",
      "Epoch 63/100: 5001batch [00:42, 116.96batch/s, Train loss=0.0192]\n",
      "Epoch 64/100: 5001batch [00:42, 116.60batch/s, Train loss=0.0191]\n",
      "Epoch 65/100: 5001batch [00:42, 117.09batch/s, Train loss=0.0193]\n",
      "Epoch 66/100: 5001batch [00:42, 116.68batch/s, Train loss=0.0195]\n",
      "Epoch 67/100: 5001batch [00:42, 116.93batch/s, Train loss=0.0191]\n",
      "Epoch 68/100: 5001batch [00:42, 116.92batch/s, Train loss=0.0194]\n",
      "Epoch 69/100: 5001batch [00:42, 116.64batch/s, Train loss=0.0189]\n",
      "Epoch 70/100: 5001batch [00:42, 117.00batch/s, Train loss=0.0188]\n",
      "Epoch 71/100: 5001batch [00:42, 116.68batch/s, Train loss=0.0185]\n",
      "Epoch 72/100: 5001batch [00:42, 116.80batch/s, Train loss=0.0187]\n",
      "Epoch 73/100: 5001batch [00:42, 116.83batch/s, Train loss=0.0184]\n",
      "Epoch 74/100: 5001batch [00:42, 117.13batch/s, Train loss=0.0187]\n",
      "Epoch 75/100: 5001batch [00:42, 116.92batch/s, Train loss=0.0187]\n",
      "Epoch 76/100: 5001batch [00:42, 116.90batch/s, Train loss=0.0183]\n",
      "Epoch 77/100: 5001batch [00:42, 116.97batch/s, Train loss=0.0184]\n",
      "Epoch 78/100: 5001batch [00:42, 117.23batch/s, Train loss=0.0182]\n",
      "Epoch 79/100: 5001batch [00:42, 116.70batch/s, Train loss=0.0184]\n",
      "Epoch 80/100: 5001batch [00:42, 116.94batch/s, Train loss=0.0185]\n",
      "Epoch 81/100: 5001batch [00:42, 116.81batch/s, Train loss=0.0183]\n",
      "Epoch 82/100: 5001batch [00:42, 116.61batch/s, Train loss=0.0182]\n",
      "Epoch 83/100: 5001batch [00:42, 116.78batch/s, Train loss=0.0182]\n",
      "Epoch 84/100: 5001batch [00:42, 116.74batch/s, Train loss=0.018]\n",
      "Epoch 85/100: 5001batch [00:42, 117.02batch/s, Train loss=0.0182]\n",
      "Epoch 86/100: 5001batch [00:39, 126.00batch/s, Train loss=0.018]\n",
      "Epoch 87/100: 5001batch [00:39, 126.50batch/s, Train loss=0.0176]\n",
      "Epoch 88/100: 5001batch [00:39, 126.01batch/s, Train loss=0.0176]\n",
      "Epoch 89/100: 5001batch [00:39, 126.32batch/s, Train loss=0.018]\n",
      "Epoch 90/100: 5001batch [00:39, 125.92batch/s, Train loss=0.0177]\n",
      "Epoch 91/100: 5001batch [00:39, 126.53batch/s, Train loss=0.0176]\n",
      "Epoch 92/100: 5001batch [00:39, 126.28batch/s, Train loss=0.0175]\n",
      "Epoch 93/100: 5001batch [00:39, 126.07batch/s, Train loss=0.0176]\n",
      "Epoch 94/100: 5001batch [00:39, 125.66batch/s, Train loss=0.0175]\n",
      "Epoch 95/100: 5001batch [00:39, 126.09batch/s, Train loss=0.0175]\n",
      "Epoch 96/100: 5001batch [00:39, 126.03batch/s, Train loss=0.017]\n",
      "Epoch 97/100: 5001batch [00:39, 126.19batch/s, Train loss=0.0173]\n",
      "Epoch 98/100: 5001batch [00:39, 126.77batch/s, Train loss=0.0173]\n",
      "Epoch 99/100: 5001batch [00:39, 126.48batch/s, Train loss=0.0177]\n",
      "Epoch 100/100: 5001batch [00:39, 126.48batch/s, Train loss=0.0174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained models have been saved as: \n",
      "ffn_model/ffn_02.h5\n"
     ]
    }
   ],
   "source": [
    "ffn_trainer.train(num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b88089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89658056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

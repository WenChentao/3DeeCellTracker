{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39685344",
   "metadata": {},
   "source": [
    "# 3DeeCellTracker Demo: Train FFN with a .csv file\n",
    "\n",
    "This notebook shows how to train a neural network called FFN for 3D cell tracking. \n",
    "\n",
    "To get started, you can download the \"worm3_points_t1.csv\" file from our GitHub repository at https://github.com/WenChentao/3DeeCellTracker/blob/master/Examples/use_stardist/worm3_points_t1.csv. This file will be used throughout the notebook to showcase the FFN training process. Alternatively, you can generate your own 3D cell coordinates in a .csv file to train your own model.\n",
    "\n",
    "**The basic procedures:**\n",
    "- A. Import packages\n",
    "- B. Initialize the trainer\n",
    "- C. Train FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc0677",
   "metadata": {},
   "source": [
    "## A. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7388e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 15:49:53.023011: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from CellTracker.ffn import TrainFFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d27445",
   "metadata": {},
   "source": [
    "## B. Initialize the trainer\n",
    "\n",
    "### Parameters\n",
    "- `points_path`: A string that specifies the path to the .csv file containing the 3D cell coordinates.\n",
    "- `model_name`: A string specifying the name of the ffn model to save. This name will be used to load the model later.\n",
    "\n",
    "### Notes:\n",
    "> By default, the trained model will be saved in the \"ffn_models\" directory. If you want to save the model in a different location, you can specify the basedir parameter and provide the directory path.\n",
    "```\n",
    "    ffn_trainer = TrainFFN(points1_path=points_path, model_name=model_name, basedir=\".\\FolderA\\FolderB\\\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d02cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 15:49:53.660878: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-04-25 15:49:53.710182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 15:49:53.711191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3080 Ti computeCapability: 8.6\n",
      "coreClock: 1.665GHz coreCount: 80 deviceMemorySize: 11.76GiB deviceMemoryBandwidth: 849.46GiB/s\n",
      "2023-04-25 15:49:53.711222: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-04-25 15:49:53.714430: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-04-25 15:49:53.714493: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-04-25 15:49:53.715420: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-25 15:49:53.715636: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-25 15:49:53.718087: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-04-25 15:49:53.718562: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-04-25 15:49:53.718652: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-04-25 15:49:53.718721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 15:49:53.719354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 15:49:53.719946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-04-25 15:49:53.720384: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 15:49:53.721291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 15:49:53.721865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3080 Ti computeCapability: 8.6\n",
      "coreClock: 1.665GHz coreCount: 80 deviceMemorySize: 11.76GiB deviceMemoryBandwidth: 849.46GiB/s\n",
      "2023-04-25 15:49:53.721908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 15:49:53.722362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 15:49:53.722784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-04-25 15:49:53.722810: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-04-25 15:49:53.957505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-04-25 15:49:53.957529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2023-04-25 15:49:53.957532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2023-04-25 15:49:53.957655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 15:49:53.958052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 15:49:53.958411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 15:49:53.958759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10033 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "points_path=\"./worm3_points_t1.csv\"\n",
    "model_name=\"ffn_worm3_0001\"\n",
    "\n",
    "ffn_trainer = TrainFFN(points1_path=points_path, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82f21b",
   "metadata": {},
   "source": [
    "## C. Train FFN\n",
    "\n",
    "### Parameters\n",
    "- `num_epochs`: An integer specifying the number of epochs for training. A larger number of epochs will require a longer training time. The default value of 100 is a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b68323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|    | 0/5000 [00:00<?, ?batch/s]2023-04-25 15:49:54.083321: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-04-25 15:49:54.380162: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-04-25 15:49:54.380194: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Epoch 1/100: 5001batch [00:38, 130.08batch/s, Train loss=0.114]\n",
      "Epoch 2/100: 5001batch [00:36, 137.56batch/s, Train loss=0.0784]\n",
      "Epoch 3/100: 5001batch [00:34, 143.19batch/s, Train loss=0.0689]\n",
      "Epoch 4/100: 5001batch [00:35, 140.81batch/s, Train loss=0.0625]\n",
      "Epoch 5/100: 5001batch [00:36, 137.74batch/s, Train loss=0.0569]\n",
      "Epoch 6/100: 5001batch [00:35, 142.68batch/s, Train loss=0.0546]\n",
      "Epoch 7/100: 5001batch [00:35, 142.66batch/s, Train loss=0.0523]\n",
      "Epoch 8/100: 5001batch [00:34, 143.25batch/s, Train loss=0.0499]\n",
      "Epoch 9/100: 5001batch [00:34, 143.30batch/s, Train loss=0.0475]\n",
      "Epoch 10/100: 5001batch [00:35, 140.27batch/s, Train loss=0.047]\n",
      "Epoch 11/100: 5001batch [00:35, 142.76batch/s, Train loss=0.0459]\n",
      "Epoch 12/100: 5001batch [00:35, 142.09batch/s, Train loss=0.0439]\n",
      "Epoch 13/100: 5001batch [00:36, 137.69batch/s, Train loss=0.0439]\n",
      "Epoch 14/100: 5001batch [00:35, 139.83batch/s, Train loss=0.0434]\n",
      "Epoch 15/100: 5001batch [00:35, 140.51batch/s, Train loss=0.0425]\n",
      "Epoch 16/100: 5001batch [00:36, 135.99batch/s, Train loss=0.0419]\n",
      "Epoch 17/100: 5001batch [00:36, 137.04batch/s, Train loss=0.041]\n",
      "Epoch 18/100: 5001batch [00:39, 126.54batch/s, Train loss=0.0411]\n",
      "Epoch 19/100: 5001batch [00:39, 126.77batch/s, Train loss=0.0406]\n",
      "Epoch 20/100: 5001batch [00:39, 126.73batch/s, Train loss=0.0396]\n",
      "Epoch 21/100: 5001batch [00:39, 126.51batch/s, Train loss=0.0401]\n",
      "Epoch 22/100: 5001batch [00:39, 126.23batch/s, Train loss=0.0388]\n",
      "Epoch 23/100: 5001batch [00:39, 126.05batch/s, Train loss=0.0384]\n",
      "Epoch 24/100: 5001batch [00:39, 126.36batch/s, Train loss=0.0384]\n",
      "Epoch 25/100: 5001batch [00:39, 126.50batch/s, Train loss=0.0375]\n",
      "Epoch 26/100: 5001batch [00:39, 126.58batch/s, Train loss=0.0367]\n",
      "Epoch 27/100: 5001batch [00:39, 126.06batch/s, Train loss=0.0373]\n",
      "Epoch 28/100: 5001batch [00:39, 126.26batch/s, Train loss=0.0357]\n",
      "Epoch 29/100: 5001batch [00:39, 126.09batch/s, Train loss=0.0361]\n",
      "Epoch 30/100: 5001batch [00:39, 126.03batch/s, Train loss=0.0361]\n",
      "Epoch 31/100: 5001batch [00:39, 125.94batch/s, Train loss=0.035]\n",
      "Epoch 32/100: 5001batch [00:39, 126.06batch/s, Train loss=0.0337]\n",
      "Epoch 33/100: 5001batch [00:39, 126.19batch/s, Train loss=0.0337]\n",
      "Epoch 34/100: 5001batch [00:39, 126.21batch/s, Train loss=0.0328]\n",
      "Epoch 35/100: 5001batch [00:39, 126.28batch/s, Train loss=0.0326]\n",
      "Epoch 36/100: 5001batch [00:39, 126.07batch/s, Train loss=0.0314]\n",
      "Epoch 37/100: 5001batch [00:39, 126.10batch/s, Train loss=0.0315]\n",
      "Epoch 38/100: 5001batch [00:39, 126.14batch/s, Train loss=0.0306]\n",
      "Epoch 39/100: 5001batch [00:39, 126.31batch/s, Train loss=0.0306]\n",
      "Epoch 40/100: 5001batch [00:39, 125.88batch/s, Train loss=0.03]\n",
      "Epoch 41/100: 5001batch [00:39, 126.33batch/s, Train loss=0.0293]\n",
      "Epoch 42/100: 5001batch [00:39, 125.88batch/s, Train loss=0.0295]\n",
      "Epoch 43/100: 5001batch [00:39, 125.84batch/s, Train loss=0.028]\n",
      "Epoch 44/100: 5001batch [00:39, 125.82batch/s, Train loss=0.0284]\n",
      "Epoch 45/100: 5001batch [00:39, 126.39batch/s, Train loss=0.0284]\n",
      "Epoch 46/100: 5001batch [00:39, 125.89batch/s, Train loss=0.0275]\n",
      "Epoch 47/100: 5001batch [00:39, 125.91batch/s, Train loss=0.0274]\n",
      "Epoch 48/100: 5001batch [00:39, 126.22batch/s, Train loss=0.0276]\n",
      "Epoch 49/100: 5001batch [00:39, 125.92batch/s, Train loss=0.0274]\n",
      "Epoch 50/100: 5001batch [00:39, 125.47batch/s, Train loss=0.0266]\n",
      "Epoch 51/100: 5001batch [00:39, 126.45batch/s, Train loss=0.027]\n",
      "Epoch 52/100: 5001batch [00:39, 126.12batch/s, Train loss=0.0268]\n",
      "Epoch 53/100: 5001batch [00:39, 125.75batch/s, Train loss=0.0266]\n",
      "Epoch 54/100: 5001batch [00:39, 125.99batch/s, Train loss=0.0263]\n",
      "Epoch 55/100: 5001batch [00:39, 126.11batch/s, Train loss=0.0256]\n",
      "Epoch 56/100: 5001batch [00:39, 125.81batch/s, Train loss=0.0258]\n",
      "Epoch 57/100: 5001batch [00:39, 125.66batch/s, Train loss=0.0257]\n",
      "Epoch 58/100: 5001batch [00:39, 125.91batch/s, Train loss=0.0257]\n",
      "Epoch 59/100: 5001batch [00:39, 125.72batch/s, Train loss=0.0249]\n",
      "Epoch 60/100: 5001batch [00:39, 126.31batch/s, Train loss=0.0251]\n",
      "Epoch 61/100: 5001batch [00:39, 126.28batch/s, Train loss=0.0255]\n",
      "Epoch 62/100: 5001batch [00:39, 126.06batch/s, Train loss=0.0248]\n",
      "Epoch 63/100: 5001batch [00:39, 125.95batch/s, Train loss=0.025]\n",
      "Epoch 64/100: 5001batch [00:39, 125.64batch/s, Train loss=0.025]\n",
      "Epoch 65/100: 5001batch [00:39, 126.28batch/s, Train loss=0.0245]\n",
      "Epoch 66/100: 5001batch [00:39, 125.65batch/s, Train loss=0.0242]\n",
      "Epoch 67/100: 5001batch [00:39, 125.73batch/s, Train loss=0.0242]\n",
      "Epoch 68/100: 5001batch [00:39, 125.62batch/s, Train loss=0.0248]\n",
      "Epoch 69/100: 5001batch [00:39, 125.95batch/s, Train loss=0.0238]\n",
      "Epoch 70/100: 5001batch [00:39, 125.95batch/s, Train loss=0.0238]\n",
      "Epoch 71/100: 5001batch [00:39, 126.00batch/s, Train loss=0.0235]\n",
      "Epoch 72/100: 5001batch [00:39, 125.71batch/s, Train loss=0.0235]\n",
      "Epoch 73/100: 5001batch [00:40, 125.01batch/s, Train loss=0.0237]\n",
      "Epoch 74/100: 5001batch [00:39, 125.12batch/s, Train loss=0.0238]\n",
      "Epoch 75/100: 5001batch [00:39, 125.34batch/s, Train loss=0.0237]\n",
      "Epoch 76/100: 5001batch [00:39, 125.59batch/s, Train loss=0.0238]\n",
      "Epoch 77/100: 5001batch [00:39, 125.69batch/s, Train loss=0.0231]\n",
      "Epoch 78/100: 5001batch [00:39, 125.74batch/s, Train loss=0.0231]\n",
      "Epoch 79/100: 5001batch [00:39, 125.66batch/s, Train loss=0.0228]\n",
      "Epoch 80/100: 5001batch [00:39, 125.19batch/s, Train loss=0.0233]\n",
      "Epoch 81/100: 5001batch [00:39, 125.42batch/s, Train loss=0.0234]\n",
      "Epoch 82/100: 5001batch [00:39, 125.27batch/s, Train loss=0.0223]\n",
      "Epoch 83/100: 5001batch [00:39, 125.22batch/s, Train loss=0.0229]\n",
      "Epoch 84/100: 5001batch [00:39, 125.55batch/s, Train loss=0.0227]\n",
      "Epoch 85/100: 5001batch [00:39, 125.41batch/s, Train loss=0.0223]\n",
      "Epoch 86/100: 5001batch [00:39, 125.22batch/s, Train loss=0.0225]\n",
      "Epoch 87/100: 5001batch [00:39, 125.29batch/s, Train loss=0.0225]\n",
      "Epoch 88/100: 5001batch [00:39, 125.24batch/s, Train loss=0.022]\n",
      "Epoch 89/100: 5001batch [00:39, 125.67batch/s, Train loss=0.0215]\n",
      "Epoch 90/100: 5001batch [00:39, 125.34batch/s, Train loss=0.0219]\n",
      "Epoch 91/100: 5001batch [00:39, 125.61batch/s, Train loss=0.0221]\n",
      "Epoch 92/100: 5001batch [00:39, 125.53batch/s, Train loss=0.0215]\n",
      "Epoch 93/100: 5001batch [00:40, 125.02batch/s, Train loss=0.0219]\n",
      "Epoch 94/100: 5001batch [00:39, 125.55batch/s, Train loss=0.0215]\n",
      "Epoch 95/100: 5001batch [00:39, 125.29batch/s, Train loss=0.0211]\n",
      "Epoch 96/100: 5001batch [00:40, 125.01batch/s, Train loss=0.021]\n",
      "Epoch 97/100: 5001batch [00:39, 125.34batch/s, Train loss=0.0215]\n",
      "Epoch 98/100: 5001batch [00:39, 125.71batch/s, Train loss=0.0208]\n",
      "Epoch 99/100: 5001batch [00:39, 125.23batch/s, Train loss=0.021]\n",
      "Epoch 100/100: 5001batch [00:39, 125.67batch/s, Train loss=0.021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained models have been saved as: \n",
      "ffn_models/ffn_worm3_0001.h5\n"
     ]
    }
   ],
   "source": [
    "num_epochs=100\n",
    "\n",
    "ffn_trainer.train(num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89658056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

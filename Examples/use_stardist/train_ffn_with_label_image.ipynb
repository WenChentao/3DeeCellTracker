{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39e286c",
   "metadata": {},
   "source": [
    "# 3DeeCellTracker Demo: Train FFN with a 3D image of segmented labels\n",
    "\n",
    "This notebook shows how to train a neural network called FFN for 3D cell tracking. \n",
    "\n",
    "The demo data used in this notebook can be found in the \"worm4\" folder, which can be downloaded from https://osf.io/pgr95/.\n",
    "\n",
    "**The basic procedures:**\n",
    "- A. Import packages\n",
    "- B. Initialize the trainer\n",
    "- C. Train FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea56535",
   "metadata": {},
   "source": [
    "## A. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7388e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 11:45:26.171082: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from CellTracker.ffn import TrainFFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec427b17",
   "metadata": {},
   "source": [
    "## B. Initialize the trainer\n",
    "\n",
    "### Parameters\n",
    "- `segmentation_path`: A string that specifies the path to the 2D images of segmented cell labels.\n",
    "- `model_name`: A string specifying the name of the ffn model to save. This name will be used to load the model later.\n",
    "- `voxel_size`: A tuple of 3 numbers, indicating the size (in arbitrary units) of a voxel in the x, y, and z directions.\n",
    "\n",
    "\n",
    "### Notes:\n",
    "> By default, the trained model will be saved in the \"ffn_models\" directory. If you want to save the model in a different location, you can specify the basedir parameter and provide the directory path.\n",
    "```\n",
    "    ffn_trainer = TrainFFN(points1_path=points_path, model_name=model_name, basedir=\".\\FolderA\\FolderB\\\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d02cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 11:45:26.821179: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-04-26 11:45:26.889626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 11:45:26.891285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3080 Ti computeCapability: 8.6\n",
      "coreClock: 1.665GHz coreCount: 80 deviceMemorySize: 11.76GiB deviceMemoryBandwidth: 849.46GiB/s\n",
      "2023-04-26 11:45:26.891328: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-04-26 11:45:26.895766: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-04-26 11:45:26.895844: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-04-26 11:45:26.897280: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-26 11:45:26.897542: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-26 11:45:26.900502: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-04-26 11:45:26.901078: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-04-26 11:45:26.901189: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-04-26 11:45:26.901268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 11:45:26.902042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 11:45:26.902742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-04-26 11:45:26.903226: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-26 11:45:26.904225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 11:45:26.904896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3080 Ti computeCapability: 8.6\n",
      "coreClock: 1.665GHz coreCount: 80 deviceMemorySize: 11.76GiB deviceMemoryBandwidth: 849.46GiB/s\n",
      "2023-04-26 11:45:26.904971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 11:45:26.905505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 11:45:26.905995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-04-26 11:45:26.906020: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-04-26 11:45:27.168228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-04-26 11:45:27.168250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2023-04-26 11:45:27.168254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2023-04-26 11:45:27.168375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 11:45:27.168761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 11:45:27.169124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-26 11:45:27.169466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10033 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "segmentation_path = \"./worm4/manual_vol1/*.tif\"\n",
    "model_name = \"ffn_worm4_0001\"\n",
    "voxel_size = (1, 1, 1)\n",
    "\n",
    "ffn_trainer = TrainFFN(model_name=model_name, segmentation1_path=segmentation_path, voxel_size=voxel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf24ae",
   "metadata": {},
   "source": [
    "## C. Train FFN\n",
    "\n",
    "### Parameters\n",
    "- `num_epochs`: An integer specifying the number of epochs for training. A larger number of epochs will require a longer training time. The default value of 100 is a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b68323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|    | 0/5000 [00:00<?, ?batch/s]2023-04-26 11:45:27.666779: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-04-26 11:45:27.994844: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-04-26 11:45:27.994883: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Epoch 1/100: 5001batch [00:38, 131.54batch/s, Train loss=0.103]\n",
      "Epoch 2/100: 5001batch [00:37, 134.58batch/s, Train loss=0.0707]\n",
      "Epoch 3/100: 5001batch [00:37, 133.78batch/s, Train loss=0.0575]\n",
      "Epoch 4/100: 5001batch [00:37, 133.16batch/s, Train loss=0.0524]\n",
      "Epoch 5/100: 5001batch [00:37, 134.18batch/s, Train loss=0.0484]\n",
      "Epoch 6/100: 5001batch [00:37, 133.86batch/s, Train loss=0.044]\n",
      "Epoch 7/100: 5001batch [00:37, 133.24batch/s, Train loss=0.042]\n",
      "Epoch 8/100: 5001batch [00:37, 132.98batch/s, Train loss=0.0409]\n",
      "Epoch 9/100: 5001batch [00:37, 133.78batch/s, Train loss=0.039]\n",
      "Epoch 10/100: 5001batch [00:37, 133.79batch/s, Train loss=0.0385]\n",
      "Epoch 11/100: 5001batch [00:37, 133.36batch/s, Train loss=0.0368]\n",
      "Epoch 12/100: 5001batch [00:37, 133.09batch/s, Train loss=0.0354]\n",
      "Epoch 13/100: 5001batch [00:37, 133.58batch/s, Train loss=0.0345]\n",
      "Epoch 14/100: 5001batch [00:37, 133.39batch/s, Train loss=0.0344]\n",
      "Epoch 15/100: 5001batch [00:37, 133.19batch/s, Train loss=0.0331]\n",
      "Epoch 16/100: 5001batch [00:40, 124.26batch/s, Train loss=0.0322]\n",
      "Epoch 17/100: 5001batch [00:40, 123.61batch/s, Train loss=0.0325]\n",
      "Epoch 18/100: 5001batch [00:40, 124.13batch/s, Train loss=0.0315]\n",
      "Epoch 19/100: 5001batch [00:40, 124.09batch/s, Train loss=0.0309]\n",
      "Epoch 20/100: 5001batch [00:40, 124.07batch/s, Train loss=0.0306]\n",
      "Epoch 21/100: 5001batch [00:40, 123.91batch/s, Train loss=0.0304]\n",
      "Epoch 22/100: 5001batch [00:40, 123.70batch/s, Train loss=0.0293]\n",
      "Epoch 23/100: 5001batch [00:40, 123.74batch/s, Train loss=0.0288]\n",
      "Epoch 24/100: 5001batch [00:40, 123.44batch/s, Train loss=0.0277]\n",
      "Epoch 25/100: 5001batch [00:40, 123.88batch/s, Train loss=0.0273]\n",
      "Epoch 26/100: 5001batch [00:40, 123.74batch/s, Train loss=0.0273]\n",
      "Epoch 27/100: 5001batch [00:40, 123.71batch/s, Train loss=0.0267]\n",
      "Epoch 28/100: 5001batch [00:40, 123.50batch/s, Train loss=0.0262]\n",
      "Epoch 29/100: 5001batch [00:40, 123.48batch/s, Train loss=0.0257]\n",
      "Epoch 30/100: 5001batch [00:40, 123.53batch/s, Train loss=0.0248]\n",
      "Epoch 31/100: 5001batch [00:40, 123.77batch/s, Train loss=0.025]\n",
      "Epoch 32/100: 5001batch [00:40, 123.66batch/s, Train loss=0.0249]\n",
      "Epoch 33/100: 5001batch [00:40, 123.27batch/s, Train loss=0.0245]\n",
      "Epoch 34/100: 5001batch [00:40, 123.73batch/s, Train loss=0.0241]\n",
      "Epoch 35/100: 5001batch [00:40, 123.98batch/s, Train loss=0.0234]\n",
      "Epoch 36/100: 5001batch [00:40, 123.90batch/s, Train loss=0.0229]\n",
      "Epoch 37/100: 5001batch [00:40, 123.75batch/s, Train loss=0.0229]\n",
      "Epoch 38/100: 5001batch [00:40, 123.61batch/s, Train loss=0.0229]\n",
      "Epoch 39/100: 5001batch [00:40, 124.36batch/s, Train loss=0.023]\n",
      "Epoch 40/100: 5001batch [00:40, 124.57batch/s, Train loss=0.0229]\n",
      "Epoch 41/100: 5001batch [00:38, 129.46batch/s, Train loss=0.0228]\n",
      "Epoch 42/100: 5001batch [00:37, 132.63batch/s, Train loss=0.022]\n",
      "Epoch 43/100: 5001batch [00:37, 132.82batch/s, Train loss=0.0224]\n",
      "Epoch 44/100: 5001batch [00:37, 132.53batch/s, Train loss=0.0223]\n",
      "Epoch 45/100: 5001batch [00:37, 133.05batch/s, Train loss=0.0217]\n",
      "Epoch 46/100: 5001batch [00:37, 132.56batch/s, Train loss=0.022]\n",
      "Epoch 47/100: 5001batch [00:37, 132.57batch/s, Train loss=0.0214]\n",
      "Epoch 48/100: 5001batch [00:37, 133.03batch/s, Train loss=0.0213]\n",
      "Epoch 49/100: 5001batch [00:37, 132.57batch/s, Train loss=0.0213]\n",
      "Epoch 50/100: 5001batch [00:37, 133.11batch/s, Train loss=0.0209]\n",
      "Epoch 51/100: 5001batch [00:37, 132.63batch/s, Train loss=0.0203]\n",
      "Epoch 52/100: 5001batch [00:37, 132.75batch/s, Train loss=0.0208]\n",
      "Epoch 53/100: 5001batch [00:37, 132.40batch/s, Train loss=0.0206]\n",
      "Epoch 54/100: 5001batch [00:37, 132.65batch/s, Train loss=0.0203]\n",
      "Epoch 55/100: 5001batch [00:37, 132.75batch/s, Train loss=0.02]\n",
      "Epoch 56/100: 5001batch [00:40, 123.33batch/s, Train loss=0.0194]\n",
      "Epoch 57/100: 5001batch [00:38, 131.58batch/s, Train loss=0.0201]\n",
      "Epoch 58/100: 5001batch [00:37, 132.65batch/s, Train loss=0.0198]\n",
      "Epoch 59/100: 5001batch [00:37, 132.23batch/s, Train loss=0.0196]\n",
      "Epoch 60/100: 5001batch [00:37, 132.59batch/s, Train loss=0.0194]\n",
      "Epoch 61/100: 5001batch [00:37, 132.96batch/s, Train loss=0.0196]\n",
      "Epoch 62/100: 5001batch [00:37, 132.98batch/s, Train loss=0.0196]\n",
      "Epoch 63/100: 5001batch [00:37, 132.40batch/s, Train loss=0.0195]\n",
      "Epoch 64/100: 5001batch [00:37, 132.57batch/s, Train loss=0.0195]\n",
      "Epoch 65/100: 5001batch [00:37, 132.30batch/s, Train loss=0.0197]\n",
      "Epoch 66/100: 5001batch [00:37, 132.51batch/s, Train loss=0.019]\n",
      "Epoch 67/100: 5001batch [00:37, 132.97batch/s, Train loss=0.0196]\n",
      "Epoch 68/100: 5001batch [00:37, 132.21batch/s, Train loss=0.0191]\n",
      "Epoch 69/100: 5001batch [00:37, 132.76batch/s, Train loss=0.0186]\n",
      "Epoch 70/100: 5001batch [00:37, 132.55batch/s, Train loss=0.0189]\n",
      "Epoch 71/100: 5001batch [00:37, 132.10batch/s, Train loss=0.0185]\n",
      "Epoch 72/100: 5001batch [00:40, 123.48batch/s, Train loss=0.0187]\n",
      "Epoch 73/100: 5001batch [00:40, 123.18batch/s, Train loss=0.019]\n",
      "Epoch 74/100: 5001batch [00:39, 127.61batch/s, Train loss=0.0185]\n",
      "Epoch 75/100: 5001batch [00:37, 132.71batch/s, Train loss=0.0182]\n",
      "Epoch 76/100: 5001batch [00:37, 132.70batch/s, Train loss=0.0188]\n",
      "Epoch 77/100: 5001batch [00:37, 131.85batch/s, Train loss=0.0188]\n",
      "Epoch 78/100: 5001batch [00:37, 132.48batch/s, Train loss=0.0181]\n",
      "Epoch 79/100: 5001batch [00:37, 132.84batch/s, Train loss=0.0182]\n",
      "Epoch 80/100: 5001batch [00:37, 132.06batch/s, Train loss=0.0178]\n",
      "Epoch 81/100: 5001batch [00:37, 132.84batch/s, Train loss=0.0178]\n",
      "Epoch 82/100: 5001batch [00:37, 131.82batch/s, Train loss=0.0184]\n",
      "Epoch 83/100: 5001batch [00:37, 131.96batch/s, Train loss=0.0178]\n",
      "Epoch 84/100: 5001batch [00:37, 132.52batch/s, Train loss=0.0174]\n",
      "Epoch 85/100: 5001batch [00:37, 132.07batch/s, Train loss=0.0177]\n",
      "Epoch 86/100: 5001batch [00:37, 132.98batch/s, Train loss=0.0176]\n",
      "Epoch 87/100: 5001batch [00:37, 132.53batch/s, Train loss=0.0177]\n",
      "Epoch 88/100: 5001batch [00:37, 132.79batch/s, Train loss=0.0176]\n",
      "Epoch 89/100: 5001batch [00:40, 123.11batch/s, Train loss=0.0172]\n",
      "Epoch 90/100: 5001batch [00:40, 123.48batch/s, Train loss=0.0172]\n",
      "Epoch 91/100: 5001batch [00:39, 127.61batch/s, Train loss=0.0171]\n",
      "Epoch 92/100: 5001batch [00:37, 132.55batch/s, Train loss=0.0175]\n",
      "Epoch 93/100: 5001batch [00:37, 132.19batch/s, Train loss=0.0171]\n",
      "Epoch 94/100: 5001batch [00:37, 132.25batch/s, Train loss=0.0168]\n",
      "Epoch 95/100: 5001batch [00:37, 132.42batch/s, Train loss=0.017]\n",
      "Epoch 96/100: 5001batch [00:37, 132.27batch/s, Train loss=0.0174]\n",
      "Epoch 97/100: 5001batch [00:37, 132.05batch/s, Train loss=0.0169]\n",
      "Epoch 98/100: 5001batch [00:37, 132.50batch/s, Train loss=0.0169]\n",
      "Epoch 99/100: 5001batch [00:37, 131.96batch/s, Train loss=0.017]\n",
      "Epoch 100/100: 5001batch [00:38, 130.90batch/s, Train loss=0.0165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained models have been saved as: \n",
      "ffn_models/ffn_worm4_0001.h5\n"
     ]
    }
   ],
   "source": [
    "num_epochs=100\n",
    "\n",
    "ffn_trainer.train(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b88089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89658056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

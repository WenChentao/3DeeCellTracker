{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7388e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CellTracker.ffn import TrainFFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d02cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 15:44:56.741256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 15:44:56.745350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 15:44:56.745760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 15:44:56.746665: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 15:44:56.747421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 15:44:56.747866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 15:44:56.748275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 15:44:57.003777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 15:44:57.004182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 15:44:57.004544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-14 15:44:57.004890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10035 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "ffn_trainer = TrainFFN(points1_path=\"./pointset_t1.csv\", points2_path=\"./pointset_t2.csv\", model_name=\"ffn_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b68323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|    | 0/5000 [00:00<?, ?batch/s]2023-04-14 15:44:57.741992: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Epoch 1/100: 5001batch [00:38, 129.78batch/s, Train loss=0.113]\n",
      "Epoch 2/100: 5001batch [00:38, 129.72batch/s, Train loss=0.0784]\n",
      "Epoch 3/100: 5001batch [00:38, 129.92batch/s, Train loss=0.0685]\n",
      "Epoch 4/100: 5001batch [00:38, 129.16batch/s, Train loss=0.0622]\n",
      "Epoch 5/100: 5001batch [00:38, 128.71batch/s, Train loss=0.0591]\n",
      "Epoch 6/100: 5001batch [00:38, 129.20batch/s, Train loss=0.0549]\n",
      "Epoch 7/100: 5001batch [00:38, 129.09batch/s, Train loss=0.0536]\n",
      "Epoch 8/100: 5001batch [00:38, 129.09batch/s, Train loss=0.0512]\n",
      "Epoch 9/100: 5001batch [00:38, 129.12batch/s, Train loss=0.0491]\n",
      "Epoch 10/100: 5001batch [00:38, 129.03batch/s, Train loss=0.0469]\n",
      "Epoch 11/100: 5001batch [00:38, 129.99batch/s, Train loss=0.0458]\n",
      "Epoch 12/100: 5001batch [00:38, 129.72batch/s, Train loss=0.0453]\n",
      "Epoch 13/100: 5001batch [00:42, 119.02batch/s, Train loss=0.043]\n",
      "Epoch 14/100: 5001batch [00:42, 118.47batch/s, Train loss=0.0433]\n",
      "Epoch 15/100: 5001batch [00:42, 118.74batch/s, Train loss=0.0429]\n",
      "Epoch 16/100: 5001batch [00:42, 118.81batch/s, Train loss=0.0412]\n",
      "Epoch 17/100: 5001batch [00:42, 118.80batch/s, Train loss=0.0415]\n",
      "Epoch 18/100: 5001batch [00:42, 118.79batch/s, Train loss=0.0413]\n",
      "Epoch 19/100: 5001batch [00:42, 118.74batch/s, Train loss=0.0399]\n",
      "Epoch 20/100: 5001batch [00:42, 118.25batch/s, Train loss=0.0405]\n",
      "Epoch 21/100: 5001batch [00:42, 118.51batch/s, Train loss=0.0394]\n",
      "Epoch 22/100: 5001batch [00:42, 118.40batch/s, Train loss=0.0392]\n",
      "Epoch 23/100: 5001batch [00:42, 118.58batch/s, Train loss=0.0376]\n",
      "Epoch 24/100: 5001batch [00:42, 118.29batch/s, Train loss=0.039]\n",
      "Epoch 25/100: 5001batch [00:42, 118.15batch/s, Train loss=0.0381]\n",
      "Epoch 26/100: 5001batch [00:42, 118.24batch/s, Train loss=0.0373]\n",
      "Epoch 27/100: 5001batch [00:42, 118.28batch/s, Train loss=0.0368]\n",
      "Epoch 28/100: 5001batch [00:41, 120.25batch/s, Train loss=0.0368]\n",
      "Epoch 29/100: 5001batch [00:38, 128.64batch/s, Train loss=0.036]\n",
      "Epoch 30/100: 5001batch [00:37, 134.99batch/s, Train loss=0.0359]\n",
      "Epoch 31/100: 5001batch [00:37, 135.10batch/s, Train loss=0.0346]\n",
      "Epoch 32/100: 5001batch [00:37, 134.88batch/s, Train loss=0.0345]\n",
      "Epoch 33/100: 5001batch [00:37, 134.54batch/s, Train loss=0.0337]\n",
      "Epoch 34/100: 5001batch [00:37, 134.32batch/s, Train loss=0.033]\n",
      "Epoch 35/100: 5001batch [00:37, 134.71batch/s, Train loss=0.032]\n",
      "Epoch 36/100: 5001batch [00:37, 134.90batch/s, Train loss=0.0314]\n",
      "Epoch 37/100: 5001batch [00:36, 135.81batch/s, Train loss=0.0307]\n",
      "Epoch 38/100: 5001batch [00:36, 135.25batch/s, Train loss=0.0309]\n",
      "Epoch 39/100: 5001batch [00:37, 134.63batch/s, Train loss=0.0299]\n",
      "Epoch 40/100: 5001batch [00:37, 134.49batch/s, Train loss=0.0294]\n",
      "Epoch 41/100: 5001batch [00:37, 134.13batch/s, Train loss=0.0293]\n",
      "Epoch 42/100: 5001batch [00:37, 134.89batch/s, Train loss=0.0292]\n",
      "Epoch 43/100: 5001batch [00:37, 134.59batch/s, Train loss=0.0283]\n",
      "Epoch 44/100: 5001batch [00:37, 135.11batch/s, Train loss=0.0281]\n",
      "Epoch 45/100: 5001batch [00:37, 134.88batch/s, Train loss=0.028]\n",
      "Epoch 46/100: 5001batch [00:37, 132.88batch/s, Train loss=0.0282]\n",
      "Epoch 47/100: 5001batch [00:39, 127.50batch/s, Train loss=0.0276]\n",
      "Epoch 48/100: 5001batch [00:39, 128.19batch/s, Train loss=0.0279]\n",
      "Epoch 49/100: 5001batch [00:38, 128.49batch/s, Train loss=0.0271]\n",
      "Epoch 50/100: 5001batch [00:39, 127.96batch/s, Train loss=0.0277]\n",
      "Epoch 51/100: 5001batch [00:38, 128.62batch/s, Train loss=0.027]\n",
      "Epoch 52/100: 5001batch [00:38, 128.61batch/s, Train loss=0.0268]\n",
      "Epoch 53/100: 5001batch [00:38, 128.46batch/s, Train loss=0.026]\n",
      "Epoch 54/100: 5001batch [00:42, 117.95batch/s, Train loss=0.0258]\n",
      "Epoch 55/100: 5001batch [00:42, 118.06batch/s, Train loss=0.0262]\n",
      "Epoch 56/100: 5001batch [00:42, 118.14batch/s, Train loss=0.0257]\n",
      "Epoch 57/100: 5001batch [00:42, 118.09batch/s, Train loss=0.0262]\n",
      "Epoch 58/100: 5001batch [00:42, 117.93batch/s, Train loss=0.0254]\n",
      "Epoch 59/100: 5001batch [00:42, 118.03batch/s, Train loss=0.0249]\n",
      "Epoch 60/100: 5001batch [00:42, 118.06batch/s, Train loss=0.0254]\n",
      "Epoch 61/100: 5001batch [00:42, 118.40batch/s, Train loss=0.0248]\n",
      "Epoch 62/100: 5001batch [00:42, 118.09batch/s, Train loss=0.0242]\n",
      "Epoch 63/100: 5001batch [00:42, 117.90batch/s, Train loss=0.0247]\n",
      "Epoch 64/100: 5001batch [00:42, 118.00batch/s, Train loss=0.0243]\n",
      "Epoch 65/100: 5001batch [00:40, 122.60batch/s, Train loss=0.0242]\n",
      "Epoch 66/100: 5001batch [00:42, 118.27batch/s, Train loss=0.0236]\n",
      "Epoch 67/100: 5001batch [00:42, 118.18batch/s, Train loss=0.0243]\n",
      "Epoch 68/100: 5001batch [00:42, 117.93batch/s, Train loss=0.024]\n",
      "Epoch 69/100: 5001batch [00:42, 118.00batch/s, Train loss=0.0237]\n",
      "Epoch 70/100: 5001batch [00:42, 117.69batch/s, Train loss=0.0242]\n",
      "Epoch 71/100: 5001batch [00:42, 117.88batch/s, Train loss=0.0235]\n",
      "Epoch 72/100: 5001batch [00:42, 118.15batch/s, Train loss=0.023]\n",
      "Epoch 73/100: 5001batch [00:42, 117.98batch/s, Train loss=0.023]\n",
      "Epoch 74/100: 5001batch [00:42, 118.00batch/s, Train loss=0.0232]\n",
      "Epoch 75/100: 5001batch [00:42, 118.16batch/s, Train loss=0.0226]\n",
      "Epoch 76/100: 5001batch [00:42, 118.01batch/s, Train loss=0.023]\n",
      "Epoch 77/100: 5001batch [00:41, 119.22batch/s, Train loss=0.0228]\n",
      "Epoch 78/100: 5001batch [00:42, 118.13batch/s, Train loss=0.0224]\n",
      "Epoch 79/100: 5001batch [00:42, 117.86batch/s, Train loss=0.0226]\n",
      "Epoch 80/100: 5001batch [00:42, 117.62batch/s, Train loss=0.0224]\n",
      "Epoch 81/100: 5001batch [00:42, 117.58batch/s, Train loss=0.0219]\n",
      "Epoch 82/100: 5001batch [00:42, 117.89batch/s, Train loss=0.0222]\n",
      "Epoch 83/100: 5001batch [00:42, 117.64batch/s, Train loss=0.0222]\n",
      "Epoch 84/100: 5001batch [00:42, 117.96batch/s, Train loss=0.0215]\n",
      "Epoch 85/100: 5001batch [00:42, 117.91batch/s, Train loss=0.0226]\n",
      "Epoch 86/100: 5001batch [00:39, 127.90batch/s, Train loss=0.0215]\n",
      "Epoch 87/100: 5001batch [00:38, 129.30batch/s, Train loss=0.0217]\n",
      "Epoch 88/100: 5001batch [00:37, 133.86batch/s, Train loss=0.0214]\n",
      "Epoch 89/100: 5001batch [00:37, 133.91batch/s, Train loss=0.0212]\n",
      "Epoch 90/100: 5001batch [00:37, 134.06batch/s, Train loss=0.021]\n",
      "Epoch 91/100: 5001batch [00:37, 133.99batch/s, Train loss=0.0216]\n",
      "Epoch 92/100: 5001batch [00:37, 133.76batch/s, Train loss=0.0209]\n",
      "Epoch 93/100: 5001batch [00:37, 134.14batch/s, Train loss=0.0205]\n",
      "Epoch 94/100: 5001batch [00:37, 133.67batch/s, Train loss=0.0206]\n",
      "Epoch 95/100: 5001batch [00:37, 134.29batch/s, Train loss=0.0204]\n",
      "Epoch 96/100: 5001batch [00:37, 133.84batch/s, Train loss=0.0205]\n",
      "Epoch 97/100: 5001batch [00:37, 134.19batch/s, Train loss=0.0203]\n",
      "Epoch 98/100: 5001batch [00:37, 133.77batch/s, Train loss=0.0209]\n",
      "Epoch 99/100: 5001batch [00:37, 134.23batch/s, Train loss=0.0206]\n",
      "Epoch 100/100: 5001batch [00:37, 133.79batch/s, Train loss=0.0201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained models have been saved as: \n",
      "ffn_model/ffn_01.h5\n"
     ]
    }
   ],
   "source": [
    "ffn_trainer.train(num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b88089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89658056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b025779",
   "metadata": {},
   "source": [
    "# 3DeeCellTracker Demo: Train FlexiblePointMatcher with a .csv file\n",
    "\n",
    "This notebook shows how to train a neural network called FlexiblePointMatching for 3D cell tracking. \n",
    "\n",
    "To get started, you can download the \"worm3_points_t1.csv\" file from our GitHub repository at https://github.com/WenChentao/3DeeCellTracker/blob/master/Examples/use_stardist/worm3_points_t1.csv. This file will be used throughout the notebook to showcase the FFN training process. Alternatively, you can generate your own 3D cell coordinates in a .csv file to train your own model.\n",
    "\n",
    "**The basic procedures:**\n",
    "- A. Import packages\n",
    "- B. Initialize the trainer\n",
    "- C. Train FPM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abecea3b",
   "metadata": {},
   "source": [
    "## A. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b11a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "from CellTracker.fpm import TrainFPM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d208af4",
   "metadata": {},
   "source": [
    "## B. Initialize the trainer\n",
    "\n",
    "### Parameters\n",
    "- `points_path`: A string that specifies the path to the .csv file containing the 3D cell coordinates.\n",
    "- `model_name`: A string specifying the name of the ffn model to save. This name will be used to load the model later.\n",
    "\n",
    "### Notes:\n",
    "> By default, the trained model will be saved in the \"ffn_models\" directory. If you want to save the model in a different location, you can specify the basedir parameter and provide the directory path.\n",
    "```\n",
    "    ffn_trainer = TrainFFN(points1_path=points_path, model_name=model_name, basedir=\".\\FolderA\\FolderB\\\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdbc042",
   "metadata": {},
   "source": [
    "## C. Train FFN\n",
    "\n",
    "### Parameters\n",
    "- `num_epochs`: An integer specifying the number of epochs for training. A larger number of epochs will require a longer training time. The default value of 100 is a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc79e3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 18:12:37.609795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 18:12:37.615401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 18:12:37.615907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 18:12:37.616733: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-30 18:12:37.617152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 18:12:37.617563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 18:12:37.617952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 18:12:37.871679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 18:12:37.872088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 18:12:37.872453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-30 18:12:37.872803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10035 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Epoch 1/100:   0%|    | 0/5000 [00:00<?, ?batch/s]2023-06-30 18:12:38.386510: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2023-06-30 18:12:39.065509: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Epoch 1/100: 5001batch [01:12, 69.41batch/s, Train loss=0.24]\n",
      "Epoch 2/100: 5001batch [01:10, 70.78batch/s, Train loss=0.172]\n",
      "Epoch 3/100: 5001batch [01:10, 70.94batch/s, Train loss=0.161]\n",
      "Epoch 4/100: 5001batch [01:10, 70.67batch/s, Train loss=0.16]\n",
      "Epoch 5/100: 5001batch [01:10, 70.55batch/s, Train loss=0.145]\n",
      "Epoch 6/100: 5001batch [01:10, 70.87batch/s, Train loss=0.14]\n",
      "Epoch 7/100: 5001batch [01:10, 70.77batch/s, Train loss=0.134]\n",
      "Epoch 8/100: 5001batch [01:10, 70.51batch/s, Train loss=0.131]\n",
      "Epoch 9/100: 5001batch [01:10, 70.55batch/s, Train loss=0.125]\n",
      "Epoch 10/100: 5001batch [01:10, 70.58batch/s, Train loss=0.12]\n",
      "Epoch 11/100: 5001batch [01:10, 70.73batch/s, Train loss=0.124]\n",
      "Epoch 12/100: 5001batch [01:11, 70.40batch/s, Train loss=0.121]\n",
      "Epoch 13/100: 5001batch [01:10, 70.51batch/s, Train loss=0.119]\n",
      "Epoch 14/100: 5001batch [01:10, 70.52batch/s, Train loss=0.117]\n",
      "Epoch 15/100: 5001batch [01:11, 70.33batch/s, Train loss=0.115]\n",
      "Epoch 16/100: 5001batch [01:10, 70.61batch/s, Train loss=0.117]\n",
      "Epoch 17/100: 5001batch [01:10, 70.53batch/s, Train loss=0.113]\n",
      "Epoch 18/100: 5001batch [01:10, 70.68batch/s, Train loss=0.113]\n",
      "Epoch 19/100: 5001batch [01:10, 70.59batch/s, Train loss=0.106]\n",
      "Epoch 20/100: 5001batch [01:15, 66.34batch/s, Train loss=0.108]\n",
      "Epoch 21/100: 5001batch [01:15, 66.49batch/s, Train loss=0.109]\n",
      "Epoch 22/100: 5001batch [01:15, 66.43batch/s, Train loss=0.102]\n",
      "Epoch 23/100: 5001batch [01:15, 66.30batch/s, Train loss=0.108]\n",
      "Epoch 24/100: 5001batch [01:15, 66.26batch/s, Train loss=0.107]\n",
      "Epoch 25/100: 5001batch [01:15, 66.41batch/s, Train loss=0.104]\n",
      "Epoch 26/100: 5001batch [01:15, 66.30batch/s, Train loss=0.105]\n",
      "Epoch 27/100: 5001batch [01:15, 66.23batch/s, Train loss=0.104]\n",
      "Epoch 28/100: 5001batch [01:15, 66.33batch/s, Train loss=0.104]\n",
      "Epoch 29/100: 5001batch [01:15, 66.19batch/s, Train loss=0.101]\n",
      "Epoch 30/100: 5001batch [01:15, 66.14batch/s, Train loss=0.0987]\n",
      "Epoch 31/100: 5001batch [01:15, 66.26batch/s, Train loss=0.0986]\n",
      "Epoch 32/100: 5001batch [01:15, 66.35batch/s, Train loss=0.1]\n",
      "Epoch 33/100: 5001batch [01:15, 66.20batch/s, Train loss=0.099]\n",
      "Epoch 34/100: 5001batch [01:15, 66.34batch/s, Train loss=0.1]\n",
      "Epoch 35/100: 5001batch [01:15, 66.38batch/s, Train loss=0.0945]\n",
      "Epoch 36/100: 5001batch [01:15, 66.39batch/s, Train loss=0.095]\n",
      "Epoch 37/100: 5001batch [01:15, 66.20batch/s, Train loss=0.0975]\n",
      "Epoch 38/100: 5001batch [01:15, 66.35batch/s, Train loss=0.0947]\n",
      "Epoch 39/100: 5001batch [01:15, 66.32batch/s, Train loss=0.0954]\n",
      "Epoch 40/100: 5001batch [01:15, 66.24batch/s, Train loss=0.092]\n",
      "Epoch 41/100: 5001batch [01:15, 66.36batch/s, Train loss=0.0917]\n",
      "Epoch 42/100: 5001batch [01:15, 66.36batch/s, Train loss=0.0906]\n",
      "Epoch 43/100: 5001batch [01:15, 66.34batch/s, Train loss=0.092]\n",
      "Epoch 44/100: 5001batch [01:15, 66.44batch/s, Train loss=0.0929]\n",
      "Epoch 45/100: 5001batch [01:15, 66.35batch/s, Train loss=0.0915]\n",
      "Epoch 46/100: 5001batch [01:15, 66.45batch/s, Train loss=0.0907]\n",
      "Epoch 47/100: 5001batch [01:15, 66.43batch/s, Train loss=0.0896]\n",
      "Epoch 48/100: 5001batch [01:15, 66.36batch/s, Train loss=0.092]\n",
      "Epoch 49/100: 5001batch [01:15, 66.37batch/s, Train loss=0.0922]\n",
      "Epoch 50/100: 5001batch [01:15, 66.34batch/s, Train loss=0.0901]\n",
      "Epoch 51/100: 5001batch [01:15, 66.40batch/s, Train loss=0.0867]\n",
      "Epoch 52/100: 5001batch [01:15, 66.53batch/s, Train loss=0.0887]\n",
      "Epoch 53/100: 5001batch [01:15, 66.38batch/s, Train loss=0.0881]\n",
      "Epoch 54/100: 5001batch [01:15, 66.47batch/s, Train loss=0.0873]\n",
      "Epoch 55/100: 5001batch [01:15, 66.38batch/s, Train loss=0.089]\n",
      "Epoch 56/100: 5001batch [01:15, 66.35batch/s, Train loss=0.0894]\n",
      "Epoch 57/100: 5001batch [01:15, 66.36batch/s, Train loss=0.0888]\n",
      "Epoch 58/100: 5001batch [01:15, 66.39batch/s, Train loss=0.0866]\n",
      "Epoch 59/100: 5001batch [01:15, 66.42batch/s, Train loss=0.0893]\n",
      "Epoch 60/100: 5001batch [01:15, 66.31batch/s, Train loss=0.0884]\n",
      "Epoch 61/100: 5001batch [01:15, 66.31batch/s, Train loss=0.0869]\n",
      "Epoch 62/100: 5001batch [01:15, 66.25batch/s, Train loss=0.0849]\n",
      "Epoch 63/100: 5001batch [01:15, 66.08batch/s, Train loss=0.0867]\n",
      "Epoch 64/100: 5001batch [01:15, 66.37batch/s, Train loss=0.0883]\n",
      "Epoch 65/100: 5001batch [01:15, 66.37batch/s, Train loss=0.0872]\n",
      "Epoch 66/100: 5001batch [01:15, 66.23batch/s, Train loss=0.0859]\n",
      "Epoch 67/100: 5001batch [01:15, 66.31batch/s, Train loss=0.0863]\n",
      "Epoch 68/100: 5001batch [01:15, 66.31batch/s, Train loss=0.085]\n",
      "Epoch 69/100: 5001batch [01:15, 66.31batch/s, Train loss=0.0883]\n",
      "Epoch 70/100: 5001batch [01:15, 66.21batch/s, Train loss=0.0878]\n",
      "Epoch 71/100: 5001batch [01:15, 66.24batch/s, Train loss=0.0856]\n",
      "Epoch 72/100: 5001batch [01:15, 66.43batch/s, Train loss=0.0857]\n",
      "Epoch 73/100: 5001batch [01:15, 66.38batch/s, Train loss=0.0868]\n",
      "Epoch 74/100: 5001batch [01:15, 66.49batch/s, Train loss=0.0874]\n",
      "Epoch 75/100: 5001batch [01:15, 66.45batch/s, Train loss=0.0828]\n",
      "Epoch 76/100: 5001batch [01:15, 66.46batch/s, Train loss=0.0855]\n",
      "Epoch 77/100: 5001batch [01:15, 66.45batch/s, Train loss=0.0817]\n",
      "Epoch 78/100: 5001batch [01:15, 66.34batch/s, Train loss=0.0849]\n",
      "Epoch 79/100: 5001batch [01:15, 66.38batch/s, Train loss=0.0875]\n",
      "Epoch 80/100: 5001batch [01:15, 66.39batch/s, Train loss=0.085]\n",
      "Epoch 81/100: 5001batch [01:15, 66.43batch/s, Train loss=0.0852]\n",
      "Epoch 82/100: 5001batch [01:15, 66.32batch/s, Train loss=0.0814]\n",
      "Epoch 83/100: 5001batch [01:15, 66.53batch/s, Train loss=0.0841]\n",
      "Epoch 84/100: 5001batch [01:15, 66.57batch/s, Train loss=0.0854]\n",
      "Epoch 85/100: 5001batch [01:15, 66.66batch/s, Train loss=0.0837]\n",
      "Epoch 86/100: 5001batch [01:15, 66.41batch/s, Train loss=0.0851]\n",
      "Epoch 87/100: 5001batch [01:15, 66.46batch/s, Train loss=0.0843]\n",
      "Epoch 88/100: 5001batch [01:15, 66.57batch/s, Train loss=0.0864]\n",
      "Epoch 89/100: 5001batch [01:15, 66.43batch/s, Train loss=0.0839]\n",
      "Epoch 90/100: 5001batch [01:15, 66.63batch/s, Train loss=0.0837]\n",
      "Epoch 91/100: 5001batch [01:15, 66.58batch/s, Train loss=0.0865]\n",
      "Epoch 92/100: 5001batch [01:15, 66.57batch/s, Train loss=0.0842]\n",
      "Epoch 93/100: 5001batch [01:15, 66.53batch/s, Train loss=0.0802]\n",
      "Epoch 94/100: 5001batch [01:15, 66.48batch/s, Train loss=0.0848]\n",
      "Epoch 95/100: 5001batch [01:15, 66.57batch/s, Train loss=0.0841]\n",
      "Epoch 96/100: 5001batch [01:15, 66.40batch/s, Train loss=0.0819]\n",
      "Epoch 97/100: 5001batch [01:15, 66.45batch/s, Train loss=0.0805]\n",
      "Epoch 98/100: 5001batch [01:15, 66.61batch/s, Train loss=0.082]\n",
      "Epoch 99/100: 5001batch [01:15, 66.45batch/s, Train loss=0.0838]\n",
      "Epoch 100/100: 5001batch [01:15, 66.56batch/s, Train loss=0.0838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained models have been saved as: \n",
      "fpm_models/fpm_cat_worm3_180_deg.h5\n"
     ]
    }
   ],
   "source": [
    "points_path=\"./worm3_points_t1.csv\"\n",
    "model_name=\"fpm_cat_worm3_180_deg\"\n",
    "deg = (-180, 180)\n",
    "\n",
    "fpm_trainer = TrainFPM(points1_path=points_path, model_type=\"cat\", model_name=model_name, range_rotation_ref=deg, range_rotation_tgt=deg)\n",
    "num_epochs=100\n",
    "\n",
    "fpm_trainer.train(num_epochs=num_epochs, iteration=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b988da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 5001batch [01:14, 67.11batch/s, Train loss=0.224]\n",
      "Epoch 2/100: 5001batch [01:14, 67.17batch/s, Train loss=0.165]\n",
      "Epoch 3/100: 5001batch [01:14, 67.07batch/s, Train loss=0.161]\n",
      "Epoch 4/100: 5001batch [01:14, 67.08batch/s, Train loss=0.151]\n",
      "Epoch 5/100: 5001batch [01:14, 67.17batch/s, Train loss=0.133]\n",
      "Epoch 6/100: 5001batch [01:14, 67.00batch/s, Train loss=0.126]\n",
      "Epoch 7/100: 5001batch [01:14, 67.05batch/s, Train loss=0.122]\n",
      "Epoch 8/100: 5001batch [01:14, 67.09batch/s, Train loss=0.122]\n",
      "Epoch 9/100: 5001batch [01:14, 67.11batch/s, Train loss=0.121]\n",
      "Epoch 10/100: 5001batch [01:14, 67.14batch/s, Train loss=0.114]\n",
      "Epoch 11/100: 5001batch [01:14, 67.11batch/s, Train loss=0.111]\n",
      "Epoch 12/100: 5001batch [01:14, 67.09batch/s, Train loss=0.11]\n",
      "Epoch 13/100: 5001batch [01:14, 67.11batch/s, Train loss=0.105]\n",
      "Epoch 14/100: 5001batch [01:14, 66.99batch/s, Train loss=0.105]\n",
      "Epoch 15/100: 5001batch [01:14, 67.11batch/s, Train loss=0.102]\n",
      "Epoch 16/100: 5001batch [01:14, 67.10batch/s, Train loss=0.101]\n",
      "Epoch 17/100: 5001batch [01:14, 67.05batch/s, Train loss=0.0993]\n",
      "Epoch 18/100: 5001batch [01:14, 67.09batch/s, Train loss=0.101]\n",
      "Epoch 19/100: 5001batch [01:14, 67.18batch/s, Train loss=0.0997]\n",
      "Epoch 20/100: 5001batch [01:14, 66.97batch/s, Train loss=0.0968]\n",
      "Epoch 21/100: 5001batch [01:14, 67.14batch/s, Train loss=0.0916]\n",
      "Epoch 22/100: 5001batch [01:14, 67.07batch/s, Train loss=0.0956]\n",
      "Epoch 23/100: 5001batch [01:14, 67.08batch/s, Train loss=0.0945]\n",
      "Epoch 24/100: 5001batch [01:14, 67.02batch/s, Train loss=0.093]\n",
      "Epoch 25/100: 5001batch [01:14, 66.98batch/s, Train loss=0.0921]\n",
      "Epoch 26/100: 5001batch [01:14, 67.04batch/s, Train loss=0.092]\n",
      "Epoch 27/100: 5001batch [01:14, 67.02batch/s, Train loss=0.0919]\n",
      "Epoch 28/100: 5001batch [01:14, 67.17batch/s, Train loss=0.0916]\n",
      "Epoch 29/100: 5001batch [01:14, 67.15batch/s, Train loss=0.0878]\n",
      "Epoch 30/100: 5001batch [01:14, 66.91batch/s, Train loss=0.0883]\n",
      "Epoch 31/100: 5001batch [01:14, 67.03batch/s, Train loss=0.0904]\n",
      "Epoch 32/100: 5001batch [01:14, 67.14batch/s, Train loss=0.0885]\n",
      "Epoch 33/100: 5001batch [01:14, 66.96batch/s, Train loss=0.0915]\n",
      "Epoch 34/100: 5001batch [01:14, 67.07batch/s, Train loss=0.0882]\n",
      "Epoch 35/100: 5001batch [01:14, 67.11batch/s, Train loss=0.087]\n",
      "Epoch 36/100: 5001batch [01:14, 67.05batch/s, Train loss=0.0897]\n",
      "Epoch 37/100: 5001batch [01:14, 66.98batch/s, Train loss=0.09]\n",
      "Epoch 38/100: 5001batch [01:14, 67.04batch/s, Train loss=0.0875]\n",
      "Epoch 39/100: 5001batch [01:14, 67.09batch/s, Train loss=0.0865]\n",
      "Epoch 40/100: 5001batch [01:14, 66.92batch/s, Train loss=0.0886]\n",
      "Epoch 41/100: 5001batch [01:14, 67.08batch/s, Train loss=0.0872]\n",
      "Epoch 42/100: 5001batch [01:14, 67.07batch/s, Train loss=0.0857]\n",
      "Epoch 43/100: 5001batch [01:14, 67.00batch/s, Train loss=0.0868]\n",
      "Epoch 44/100: 5001batch [01:14, 67.07batch/s, Train loss=0.0861]\n",
      "Epoch 45/100: 5001batch [01:14, 67.11batch/s, Train loss=0.0845]\n",
      "Epoch 46/100: 5001batch [01:14, 67.01batch/s, Train loss=0.0852]\n",
      "Epoch 47/100: 5001batch [01:14, 66.99batch/s, Train loss=0.0857]\n",
      "Epoch 48/100: 5001batch [01:14, 66.99batch/s, Train loss=0.0843]\n",
      "Epoch 49/100: 5001batch [01:14, 66.92batch/s, Train loss=0.0848]\n",
      "Epoch 50/100: 5001batch [01:14, 66.94batch/s, Train loss=0.0826]\n",
      "Epoch 51/100: 5001batch [01:14, 67.01batch/s, Train loss=0.0832]\n",
      "Epoch 52/100: 5001batch [01:14, 67.12batch/s, Train loss=0.0813]\n",
      "Epoch 53/100: 5001batch [01:14, 66.97batch/s, Train loss=0.0818]\n",
      "Epoch 54/100: 5001batch [01:14, 66.97batch/s, Train loss=0.0818]\n",
      "Epoch 55/100: 5001batch [01:14, 67.07batch/s, Train loss=0.0821]\n",
      "Epoch 56/100: 5001batch [01:14, 66.97batch/s, Train loss=0.0794]\n",
      "Epoch 57/100: 5001batch [01:14, 66.85batch/s, Train loss=0.0804]\n",
      "Epoch 58/100: 5001batch [01:14, 66.94batch/s, Train loss=0.0793]\n",
      "Epoch 59/100: 5001batch [01:14, 67.03batch/s, Train loss=0.0808]\n",
      "Epoch 60/100: 5001batch [01:14, 66.92batch/s, Train loss=0.0799]\n",
      "Epoch 61/100: 5001batch [01:14, 66.94batch/s, Train loss=0.0776]\n",
      "Epoch 62/100: 5001batch [01:14, 66.90batch/s, Train loss=0.0781]\n",
      "Epoch 63/100: 5001batch [01:14, 66.82batch/s, Train loss=0.0779]\n",
      "Epoch 64/100: 5001batch [01:14, 66.87batch/s, Train loss=0.0788]\n",
      "Epoch 65/100: 5001batch [01:14, 67.02batch/s, Train loss=0.0787]\n",
      "Epoch 66/100: 5001batch [01:14, 66.90batch/s, Train loss=0.078]\n",
      "Epoch 67/100: 5001batch [01:14, 66.94batch/s, Train loss=0.0748]\n",
      "Epoch 68/100: 5001batch [01:14, 66.95batch/s, Train loss=0.076]\n",
      "Epoch 69/100: 5001batch [01:14, 66.87batch/s, Train loss=0.0746]\n",
      "Epoch 70/100: 5001batch [01:14, 66.71batch/s, Train loss=0.08]\n",
      "Epoch 71/100: 5001batch [01:14, 66.93batch/s, Train loss=0.0782]\n",
      "Epoch 72/100: 5001batch [01:14, 66.83batch/s, Train loss=0.0765]\n",
      "Epoch 73/100: 5001batch [01:14, 66.79batch/s, Train loss=0.0732]\n",
      "Epoch 74/100: 5001batch [01:14, 66.90batch/s, Train loss=0.0742]\n",
      "Epoch 75/100: 5001batch [01:14, 66.91batch/s, Train loss=0.0755]\n",
      "Epoch 76/100: 5001batch [01:14, 66.80batch/s, Train loss=0.0712]\n",
      "Epoch 77/100: 5001batch [01:14, 66.94batch/s, Train loss=0.0754]\n",
      "Epoch 78/100: 5001batch [01:14, 66.78batch/s, Train loss=0.0752]\n",
      "Epoch 79/100: 5001batch [01:14, 66.81batch/s, Train loss=0.0746]\n",
      "Epoch 80/100: 5001batch [01:14, 66.81batch/s, Train loss=0.0744]\n",
      "Epoch 81/100: 5001batch [01:14, 66.84batch/s, Train loss=0.0719]\n",
      "Epoch 82/100: 5001batch [01:14, 66.71batch/s, Train loss=0.0739]\n",
      "Epoch 83/100: 5001batch [01:14, 66.78batch/s, Train loss=0.0736]\n",
      "Epoch 84/100: 5001batch [01:14, 66.90batch/s, Train loss=0.072]\n",
      "Epoch 85/100: 5001batch [01:14, 66.86batch/s, Train loss=0.073]\n",
      "Epoch 86/100: 5001batch [01:14, 66.72batch/s, Train loss=0.0709]\n",
      "Epoch 87/100: 5001batch [01:14, 66.92batch/s, Train loss=0.0724]\n",
      "Epoch 88/100: 5001batch [01:14, 66.84batch/s, Train loss=0.0731]\n",
      "Epoch 89/100: 5001batch [01:14, 66.83batch/s, Train loss=0.0731]\n",
      "Epoch 90/100: 5001batch [01:14, 67.01batch/s, Train loss=0.0732]\n",
      "Epoch 91/100: 5001batch [01:14, 66.98batch/s, Train loss=0.0717]\n",
      "Epoch 92/100: 5001batch [01:14, 67.01batch/s, Train loss=0.0727]\n",
      "Epoch 93/100: 5001batch [01:14, 66.94batch/s, Train loss=0.0697]\n",
      "Epoch 94/100: 5001batch [01:14, 66.92batch/s, Train loss=0.0716]\n",
      "Epoch 95/100: 5001batch [01:14, 66.91batch/s, Train loss=0.0724]\n",
      "Epoch 96/100: 5001batch [01:14, 66.90batch/s, Train loss=0.0716]\n",
      "Epoch 97/100: 5001batch [01:14, 66.96batch/s, Train loss=0.072]\n",
      "Epoch 98/100: 5001batch [01:14, 66.98batch/s, Train loss=0.0711]\n",
      "Epoch 99/100: 5001batch [01:14, 66.93batch/s, Train loss=0.0722]\n",
      "Epoch 100/100: 5001batch [01:14, 67.04batch/s, Train loss=0.0703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained models have been saved as: \n",
      "fpm_models/fpm_add_worm3_180_deg.h5\n"
     ]
    }
   ],
   "source": [
    "points_path=\"./worm3_points_t1.csv\"\n",
    "model_name=\"fpm_add_worm3_180_deg\"\n",
    "deg = (-180, 180)\n",
    "\n",
    "fpm_trainer = TrainFPM(points1_path=points_path, model_type=\"add\", model_name=model_name, range_rotation_ref=deg, range_rotation_tgt=deg)\n",
    "num_epochs=100\n",
    "\n",
    "fpm_trainer.train(num_epochs=num_epochs, iteration=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b7f7cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 5001batch [00:58, 85.71batch/s, Train loss=0.259]\n",
      "Epoch 2/100: 5001batch [00:58, 85.91batch/s, Train loss=0.203]\n",
      "Epoch 3/100: 5001batch [00:58, 85.95batch/s, Train loss=0.189]\n",
      "Epoch 4/100: 5001batch [00:58, 85.98batch/s, Train loss=0.181]\n",
      "Epoch 5/100: 5001batch [00:58, 85.83batch/s, Train loss=0.174]\n",
      "Epoch 6/100: 5001batch [00:58, 85.82batch/s, Train loss=0.172]\n",
      "Epoch 7/100: 5001batch [00:58, 85.83batch/s, Train loss=0.164]\n",
      "Epoch 8/100: 5001batch [00:58, 85.85batch/s, Train loss=0.16]\n",
      "Epoch 9/100: 5001batch [00:58, 85.99batch/s, Train loss=0.153]\n",
      "Epoch 10/100: 5001batch [00:58, 85.67batch/s, Train loss=0.149]\n",
      "Epoch 11/100: 5001batch [00:58, 85.80batch/s, Train loss=0.15]\n",
      "Epoch 12/100: 5001batch [00:58, 85.93batch/s, Train loss=0.148]\n",
      "Epoch 13/100: 5001batch [00:58, 85.95batch/s, Train loss=0.146]\n",
      "Epoch 14/100: 5001batch [00:58, 85.82batch/s, Train loss=0.143]\n",
      "Epoch 15/100: 5001batch [00:58, 85.98batch/s, Train loss=0.143]\n",
      "Epoch 16/100: 5001batch [00:58, 85.91batch/s, Train loss=0.146]\n",
      "Epoch 17/100: 5001batch [00:58, 85.73batch/s, Train loss=0.142]\n",
      "Epoch 18/100: 5001batch [00:58, 85.87batch/s, Train loss=0.14]\n",
      "Epoch 19/100: 5001batch [00:58, 85.75batch/s, Train loss=0.14]\n",
      "Epoch 20/100: 5001batch [00:58, 85.78batch/s, Train loss=0.134]\n",
      "Epoch 21/100: 5001batch [00:58, 85.76batch/s, Train loss=0.129]\n",
      "Epoch 22/100: 5001batch [00:58, 85.80batch/s, Train loss=0.127]\n",
      "Epoch 23/100: 5001batch [00:58, 85.83batch/s, Train loss=0.125]\n",
      "Epoch 24/100: 5001batch [00:58, 85.67batch/s, Train loss=0.128]\n",
      "Epoch 25/100: 5001batch [00:58, 85.83batch/s, Train loss=0.125]\n",
      "Epoch 26/100: 5001batch [00:58, 85.79batch/s, Train loss=0.125]\n",
      "Epoch 27/100: 5001batch [00:58, 85.57batch/s, Train loss=0.124]\n",
      "Epoch 28/100: 5001batch [00:58, 85.92batch/s, Train loss=0.12]\n",
      "Epoch 29/100: 5001batch [00:58, 85.68batch/s, Train loss=0.117]\n",
      "Epoch 30/100: 5001batch [00:58, 85.57batch/s, Train loss=0.116]\n",
      "Epoch 31/100: 5001batch [00:58, 85.64batch/s, Train loss=0.12]\n",
      "Epoch 32/100: 5001batch [00:58, 85.68batch/s, Train loss=0.118]\n",
      "Epoch 33/100: 5001batch [00:58, 85.57batch/s, Train loss=0.117]\n",
      "Epoch 34/100: 5001batch [00:58, 85.54batch/s, Train loss=0.118]\n",
      "Epoch 35/100: 5001batch [00:58, 85.92batch/s, Train loss=0.113]\n",
      "Epoch 36/100: 5001batch [00:58, 85.72batch/s, Train loss=0.116]\n",
      "Epoch 37/100: 5001batch [00:58, 85.54batch/s, Train loss=0.115]\n",
      "Epoch 38/100: 5001batch [00:58, 85.73batch/s, Train loss=0.114]\n",
      "Epoch 39/100: 5001batch [00:58, 85.70batch/s, Train loss=0.114]\n",
      "Epoch 40/100: 5001batch [00:58, 85.65batch/s, Train loss=0.115]\n",
      "Epoch 41/100: 5001batch [00:58, 85.75batch/s, Train loss=0.116]\n",
      "Epoch 42/100: 5001batch [00:58, 85.79batch/s, Train loss=0.112]\n",
      "Epoch 43/100: 5001batch [00:58, 85.53batch/s, Train loss=0.114]\n",
      "Epoch 44/100: 5001batch [00:58, 85.81batch/s, Train loss=0.113]\n",
      "Epoch 45/100: 5001batch [00:58, 85.74batch/s, Train loss=0.116]\n",
      "Epoch 46/100: 5001batch [00:58, 85.64batch/s, Train loss=0.111]\n",
      "Epoch 47/100: 5001batch [00:58, 85.60batch/s, Train loss=0.111]\n",
      "Epoch 48/100: 5001batch [00:58, 85.71batch/s, Train loss=0.108]\n",
      "Epoch 49/100: 5001batch [00:58, 85.64batch/s, Train loss=0.112]\n",
      "Epoch 50/100: 5001batch [00:58, 85.47batch/s, Train loss=0.108]\n",
      "Epoch 51/100: 5001batch [00:58, 85.60batch/s, Train loss=0.112]\n",
      "Epoch 52/100: 5001batch [00:58, 85.64batch/s, Train loss=0.111]\n",
      "Epoch 53/100: 5001batch [00:58, 85.61batch/s, Train loss=0.111]\n",
      "Epoch 54/100: 5001batch [00:58, 85.66batch/s, Train loss=0.108]\n",
      "Epoch 55/100: 5001batch [00:58, 85.73batch/s, Train loss=0.11]\n",
      "Epoch 56/100: 5001batch [00:58, 85.67batch/s, Train loss=0.108]\n",
      "Epoch 57/100: 5001batch [00:58, 85.58batch/s, Train loss=0.109]\n",
      "Epoch 58/100: 5001batch [00:58, 85.60batch/s, Train loss=0.106]\n",
      "Epoch 59/100: 5001batch [00:58, 85.70batch/s, Train loss=0.105]\n",
      "Epoch 60/100: 5001batch [00:58, 85.57batch/s, Train loss=0.11]\n",
      "Epoch 61/100: 5001batch [00:58, 85.64batch/s, Train loss=0.108]\n",
      "Epoch 62/100: 5001batch [00:58, 85.65batch/s, Train loss=0.109]\n",
      "Epoch 63/100: 5001batch [00:58, 85.62batch/s, Train loss=0.107]\n",
      "Epoch 64/100: 5001batch [00:58, 85.70batch/s, Train loss=0.107]\n",
      "Epoch 65/100: 5001batch [00:58, 85.58batch/s, Train loss=0.107]\n",
      "Epoch 66/100: 5001batch [00:58, 85.27batch/s, Train loss=0.107]\n",
      "Epoch 67/100: 5001batch [00:58, 85.67batch/s, Train loss=0.106]\n",
      "Epoch 68/100: 5001batch [00:58, 85.69batch/s, Train loss=0.106]\n",
      "Epoch 69/100: 5001batch [00:58, 85.66batch/s, Train loss=0.106]\n",
      "Epoch 70/100: 5001batch [00:58, 85.55batch/s, Train loss=0.106]\n",
      "Epoch 71/100: 5001batch [00:58, 85.69batch/s, Train loss=0.103]\n",
      "Epoch 72/100: 5001batch [00:58, 85.63batch/s, Train loss=0.105]\n",
      "Epoch 73/100: 5001batch [00:58, 85.68batch/s, Train loss=0.105]\n",
      "Epoch 74/100: 5001batch [00:58, 85.58batch/s, Train loss=0.103]\n",
      "Epoch 75/100: 5001batch [00:58, 85.55batch/s, Train loss=0.104]\n",
      "Epoch 76/100: 5001batch [00:58, 85.48batch/s, Train loss=0.102]\n",
      "Epoch 77/100: 5001batch [00:58, 85.67batch/s, Train loss=0.107]\n",
      "Epoch 78/100: 5001batch [00:58, 85.47batch/s, Train loss=0.105]\n",
      "Epoch 79/100: 5001batch [00:58, 85.52batch/s, Train loss=0.105]\n",
      "Epoch 80/100: 5001batch [00:58, 85.35batch/s, Train loss=0.106]\n",
      "Epoch 81/100: 5001batch [00:58, 85.60batch/s, Train loss=0.104]\n",
      "Epoch 82/100: 5001batch [00:58, 85.69batch/s, Train loss=0.107]\n",
      "Epoch 83/100: 5001batch [00:58, 85.46batch/s, Train loss=0.104]\n",
      "Epoch 84/100: 5001batch [00:58, 85.77batch/s, Train loss=0.101]\n",
      "Epoch 85/100: 5001batch [00:58, 85.66batch/s, Train loss=0.105]\n",
      "Epoch 86/100: 5001batch [00:58, 85.51batch/s, Train loss=0.106]\n",
      "Epoch 87/100: 5001batch [00:58, 85.54batch/s, Train loss=0.1]\n",
      "Epoch 88/100: 5001batch [00:58, 85.65batch/s, Train loss=0.103]\n",
      "Epoch 89/100: 5001batch [00:58, 85.29batch/s, Train loss=0.103]\n",
      "Epoch 90/100: 5001batch [00:58, 85.57batch/s, Train loss=0.101]\n",
      "Epoch 91/100: 5001batch [00:58, 85.55batch/s, Train loss=0.102]\n",
      "Epoch 92/100: 5001batch [00:58, 85.46batch/s, Train loss=0.102]\n",
      "Epoch 93/100: 5001batch [00:58, 85.38batch/s, Train loss=0.0991]\n",
      "Epoch 94/100: 5001batch [00:58, 85.74batch/s, Train loss=0.102]\n",
      "Epoch 95/100: 5001batch [00:58, 85.67batch/s, Train loss=0.101]\n",
      "Epoch 96/100: 5001batch [00:58, 85.45batch/s, Train loss=0.101]\n",
      "Epoch 97/100: 5001batch [00:58, 85.64batch/s, Train loss=0.103]\n",
      "Epoch 98/100: 5001batch [00:58, 85.57batch/s, Train loss=0.102]\n",
      "Epoch 99/100: 5001batch [00:58, 85.43batch/s, Train loss=0.101]\n",
      "Epoch 100/100: 5001batch [00:58, 85.82batch/s, Train loss=0.103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained models have been saved as: \n",
      "fpm_models/fpm_ori_worm3_180_deg.h5\n"
     ]
    }
   ],
   "source": [
    "points_path=\"./worm3_points_t1.csv\"\n",
    "model_name=\"fpm_ori_worm3_180_deg\"\n",
    "deg = (-180, 180)\n",
    "\n",
    "fpm_trainer = TrainFPM(points1_path=points_path, model_type=\"original\", model_name=model_name, range_rotation_ref=deg, range_rotation_tgt=deg)\n",
    "num_epochs=100\n",
    "\n",
    "fpm_trainer.train(num_epochs=num_epochs, iteration=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848db3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
